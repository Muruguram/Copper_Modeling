{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Data Preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from scipy.stats import skew\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#read csv file and tranformed into dataframe \n",
    "\n",
    "df=pd.read_csv('Copper_Set.csv',low_memory=False)\n",
    "df.info()\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 181673 entries, 0 to 181672\n",
    "Data columns (total 14 columns):\n",
    " #   Column         Non-Null Count   Dtype  \n",
    "---  ------         --------------   -----  \n",
    " 0   id             181671 non-null  object \n",
    " 1   item_date      181672 non-null  float64\n",
    " 2   quantity tons  181673 non-null  object \n",
    " 3   customer       181672 non-null  float64\n",
    " 4   country        181645 non-null  float64\n",
    " 5   status         181671 non-null  object \n",
    " 6   item type      181673 non-null  object \n",
    " 7   application    181649 non-null  float64\n",
    " 8   thickness      181672 non-null  float64\n",
    " 9   width          181673 non-null  float64\n",
    " 10  material_ref   103754 non-null  object \n",
    " 11  product_ref    181673 non-null  int64  \n",
    " 12  delivery date  181672 non-null  float64\n",
    " 13  selling_price  181672 non-null  float64\n",
    "dtypes: float64(8), int64(1), object(5)\n",
    "memory usage: 19.4+ MB\n",
    "df.head()\n",
    "id\titem_date\tquantity tons\tcustomer\tcountry\tstatus\titem type\tapplication\tthickness\twidth\tmaterial_ref\tproduct_ref\tdelivery date\tselling_price\n",
    "0\tEC06F063-9DF0-440C-8764-0B0C05A4F6AE\t20210401.0\t54.15113862\t30156308.0\t28.0\tWon\tW\t10.0\t2.00\t1500.0\tDEQ1 S460MC\t1670798778\t20210701.0\t854.00\n",
    "1\t4E5F4B3D-DDDF-499D-AFDE-A3227EC49425\t20210401.0\t768.0248392\t30202938.0\t25.0\tWon\tW\t41.0\t0.80\t1210.0\t0000000000000000000000000000000000104991\t1668701718\t20210401.0\t1047.00\n",
    "2\tE140FF1B-2407-4C02-A0DD-780A093B1158\t20210401.0\t386.1279489\t30153963.0\t30.0\tWon\tWI\t28.0\t0.38\t952.0\tS0380700\t628377\t20210101.0\t644.33\n",
    "3\tF8D507A0-9C62-4EFE-831E-33E1DA53BB50\t20210401.0\t202.4110654\t30349574.0\t32.0\tWon\tS\t59.0\t2.30\t1317.0\tDX51D+ZM310MAO 2.3X1317\t1668701718\t20210101.0\t768.00\n",
    "4\t4E1C4E78-152B-430A-8094-ADD889C9D0AD\t20210401.0\t785.5262616\t30211560.0\t28.0\tWon\tW\t10.0\t4.00\t2000.0\t2_S275JR+AR-CL1\t640665\t20210301.0\t577.00\n",
    "# verify the unique items in every features\n",
    "\n",
    "for i in list(df.columns):\n",
    "    print(f\"{i} : {df[i].nunique()}\")\n",
    "id : 181671\n",
    "item_date : 252\n",
    "quantity tons : 181673\n",
    "customer : 1169\n",
    "country : 17\n",
    "status : 9\n",
    "item type : 7\n",
    "application : 30\n",
    "thickness : 594\n",
    "width : 1386\n",
    "material_ref : 16563\n",
    "product_ref : 33\n",
    "delivery date : 28\n",
    "selling_price : 9795\n",
    "#convert the data type from object to numeric\n",
    "\n",
    "df['quantity tons'] = pd.to_numeric(df['quantity tons'], errors='coerce')\n",
    "df['customer'] = pd.to_numeric(df['customer'], errors='coerce')\n",
    "df['country'] = pd.to_numeric(df['country'], errors='coerce')\n",
    "\n",
    "df['item_date'] = pd.to_datetime(df['item_date'], format='%Y%m%d', errors='coerce').dt.date\n",
    "df['delivery date'] = pd.to_datetime(df['delivery date'], format='%Y%m%d', errors='coerce').dt.date\n",
    "# make additional feature from date's columns \n",
    "\n",
    "df['item_date'] = pd.to_datetime(df['item_date'])\n",
    "df['delivery date'] = pd.to_datetime(df['delivery date'])\n",
    "df['delivery_time_taken']=(df['item_date']-df['delivery date']).abs().dt.days\n",
    "#verify the data type of each features\n",
    "\n",
    "df.dtypes\n",
    "id                             object\n",
    "item_date              datetime64[ns]\n",
    "quantity tons                 float64\n",
    "customer                      float64\n",
    "country                       float64\n",
    "status                         object\n",
    "item type                      object\n",
    "application                   float64\n",
    "thickness                     float64\n",
    "width                         float64\n",
    "material_ref                   object\n",
    "product_ref                     int64\n",
    "delivery date          datetime64[ns]\n",
    "selling_price                 float64\n",
    "delivery_time_taken           float64\n",
    "dtype: object\n",
    "#checking null values in each features\n",
    "\n",
    "df.isnull().sum()\n",
    "id                         2\n",
    "item_date                  3\n",
    "quantity tons              1\n",
    "customer                   1\n",
    "country                   28\n",
    "status                     2\n",
    "item type                  0\n",
    "application               24\n",
    "thickness                  1\n",
    "width                      0\n",
    "material_ref           77919\n",
    "product_ref                0\n",
    "delivery date              3\n",
    "selling_price              1\n",
    "delivery_time_taken        6\n",
    "dtype: int64\n",
    "# handling some unwanted values start with ('00000') and checking null values\n",
    "\n",
    "df['material_ref'] = df['material_ref'].apply(lambda x: np.nan if str(x).startswith('00000') else x)\n",
    "df.isnull().sum()\n",
    "id                          2\n",
    "item_date                   3\n",
    "quantity tons               1\n",
    "customer                    1\n",
    "country                    28\n",
    "status                      2\n",
    "item type                   0\n",
    "application                24\n",
    "thickness                   1\n",
    "width                       0\n",
    "material_ref           100645\n",
    "product_ref                 0\n",
    "delivery date               3\n",
    "selling_price               1\n",
    "delivery_time_taken         6\n",
    "dtype: int64\n",
    "# droping the columns\n",
    "\n",
    "df.drop(columns=['id','item_date','delivery date','material_ref'], inplace=True)\n",
    "df\n",
    "quantity tons\tcustomer\tcountry\tstatus\titem type\tapplication\tthickness\twidth\tproduct_ref\tselling_price\tdelivery_time_taken\n",
    "0\t54.151139\t30156308.0\t28.0\tWon\tW\t10.0\t2.00\t1500.0\t1670798778\t854.00\t91.0\n",
    "1\t768.024839\t30202938.0\t25.0\tWon\tW\t41.0\t0.80\t1210.0\t1668701718\t1047.00\t0.0\n",
    "2\t386.127949\t30153963.0\t30.0\tWon\tWI\t28.0\t0.38\t952.0\t628377\t644.33\t90.0\n",
    "3\t202.411065\t30349574.0\t32.0\tWon\tS\t59.0\t2.30\t1317.0\t1668701718\t768.00\t90.0\n",
    "4\t785.526262\t30211560.0\t28.0\tWon\tW\t10.0\t4.00\t2000.0\t640665\t577.00\t31.0\n",
    "...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\n",
    "181668\t102.482422\t30200854.0\t25.0\tWon\tW\t41.0\t0.96\t1220.0\t164141591\t591.00\t1.0\n",
    "181669\t208.086469\t30200854.0\t25.0\tWon\tW\t41.0\t0.95\t1500.0\t164141591\t589.00\t1.0\n",
    "181670\t4.235594\t30200854.0\t25.0\tWon\tW\t41.0\t0.71\t1250.0\t164141591\t619.00\t1.0\n",
    "181671\t-2000.000000\t30200854.0\t25.0\tWon\tW\t41.0\t0.85\t1250.0\t164141591\t601.00\t1.0\n",
    "181672\t406.686538\t30200854.0\t25.0\tWon\tW\t41.0\t0.71\t1240.0\t164141591\t607.00\t1.0\n",
    "181673 rows × 11 columns\n",
    "\n",
    "df.describe().T\n",
    "count\tmean\tstd\tmin\t25%\t50%\t75%\tmax\n",
    "quantity tons\t181672.0\t5.874926e+03\t2.349081e+06\t-2000.00\t1.097030e+01\t3.036464e+01\t6.716061e+01\t1.000000e+09\n",
    "customer\t181672.0\t3.051221e+07\t2.433382e+07\t12458.00\t3.019688e+07\t3.020524e+07\t3.028042e+07\t2.147484e+09\n",
    "country\t181645.0\t4.489302e+01\t2.440421e+01\t25.00\t2.600000e+01\t3.000000e+01\t7.800000e+01\t1.130000e+02\n",
    "application\t181649.0\t2.561581e+01\t1.775417e+01\t2.00\t1.000000e+01\t1.500000e+01\t4.100000e+01\t9.900000e+01\n",
    "thickness\t181672.0\t2.564827e+00\t6.572321e+00\t0.18\t7.000000e-01\t1.500000e+00\t3.000000e+00\t2.500000e+03\n",
    "width\t181673.0\t1.295287e+03\t2.616318e+02\t1.00\t1.180000e+03\t1.250000e+03\t1.500000e+03\t2.990000e+03\n",
    "product_ref\t181673.0\t4.739679e+08\t7.175101e+08\t611728.00\t6.119930e+05\t6.406650e+05\t1.332077e+09\t1.722208e+09\n",
    "selling_price\t181672.0\t1.918036e+03\t3.317956e+05\t-1160.00\t6.690000e+02\t8.120000e+02\t9.530000e+02\t1.000010e+08\n",
    "delivery_time_taken\t181667.0\t6.413128e+01\t4.095861e+01\t0.00\t2.900000e+01\t6.100000e+01\t9.700000e+01\t6.890000e+02\n",
    "# quantity and selling price values are not below 0. so we convert to null for below 0 value\n",
    "\n",
    "df['quantity tons'] = df['quantity tons'].apply(lambda x: np.nan if x<=0 else x)\n",
    "df['selling_price'] = df['selling_price'].apply(lambda x: np.nan if x<=0 else x)\n",
    "df.describe().T\n",
    "count\tmean\tstd\tmin\t25%\t50%\t75%\tmax\n",
    "quantity tons\t181668.0\t5.875066e+03\t2.349107e+06\t0.00001\t1.097122e+01\t3.036501e+01\t6.716134e+01\t1.000000e+09\n",
    "customer\t181672.0\t3.051221e+07\t2.433382e+07\t12458.00000\t3.019688e+07\t3.020524e+07\t3.028042e+07\t2.147484e+09\n",
    "country\t181645.0\t4.489302e+01\t2.440421e+01\t25.00000\t2.600000e+01\t3.000000e+01\t7.800000e+01\t1.130000e+02\n",
    "application\t181649.0\t2.561581e+01\t1.775417e+01\t2.00000\t1.000000e+01\t1.500000e+01\t4.100000e+01\t9.900000e+01\n",
    "thickness\t181672.0\t2.564827e+00\t6.572321e+00\t0.18000\t7.000000e-01\t1.500000e+00\t3.000000e+00\t2.500000e+03\n",
    "width\t181673.0\t1.295287e+03\t2.616318e+02\t1.00000\t1.180000e+03\t1.250000e+03\t1.500000e+03\t2.990000e+03\n",
    "product_ref\t181673.0\t4.739679e+08\t7.175101e+08\t611728.00000\t6.119930e+05\t6.406650e+05\t1.332077e+09\t1.722208e+09\n",
    "selling_price\t181665.0\t1.918124e+03\t3.318020e+05\t0.10000\t6.690000e+02\t8.120000e+02\t9.530000e+02\t1.000010e+08\n",
    "delivery_time_taken\t181667.0\t6.413128e+01\t4.095861e+01\t0.00000\t2.900000e+01\t6.100000e+01\t9.700000e+01\t6.890000e+02\n",
    "df.isnull().sum()\n",
    "quantity tons           5\n",
    "customer                1\n",
    "country                28\n",
    "status                  2\n",
    "item type               0\n",
    "application            24\n",
    "thickness               1\n",
    "width                   0\n",
    "product_ref             0\n",
    "selling_price           8\n",
    "delivery_time_taken     6\n",
    "dtype: int64\n",
    "# Handling null values using median and mode\n",
    "\n",
    "df['status'].fillna(df['status'].mode().iloc[0],inplace=True)\n",
    "\n",
    "df['quantity tons'].fillna(df['quantity tons'].median(),inplace=True)\n",
    "df['customer'].fillna(df['customer'].median(),inplace=True)\n",
    "df['country'].fillna(df['country'].median(),inplace=True)\n",
    "df['application'].fillna(df['application'].median(),inplace=True)\n",
    "df['thickness'].fillna(df['thickness'].median(),inplace=True)\n",
    "df['selling_price'].fillna(df['selling_price'].median(),inplace=True)\n",
    "df['delivery_time_taken'].fillna(df['delivery_time_taken'].median(),inplace=True)\n",
    "# verify null values for each features\n",
    "\n",
    "df.isnull().sum()\n",
    "quantity tons          0\n",
    "customer               0\n",
    "country                0\n",
    "status                 0\n",
    "item type              0\n",
    "application            0\n",
    "thickness              0\n",
    "width                  0\n",
    "product_ref            0\n",
    "selling_price          0\n",
    "delivery_time_taken    0\n",
    "dtype: int64\n",
    "df\n",
    "quantity tons\tcustomer\tcountry\tstatus\titem type\tapplication\tthickness\twidth\tproduct_ref\tselling_price\tdelivery_time_taken\n",
    "0\t54.151139\t30156308.0\t28.0\tWon\tW\t10.0\t2.00\t1500.0\t1670798778\t854.00\t91.0\n",
    "1\t768.024839\t30202938.0\t25.0\tWon\tW\t41.0\t0.80\t1210.0\t1668701718\t1047.00\t0.0\n",
    "2\t386.127949\t30153963.0\t30.0\tWon\tWI\t28.0\t0.38\t952.0\t628377\t644.33\t90.0\n",
    "3\t202.411065\t30349574.0\t32.0\tWon\tS\t59.0\t2.30\t1317.0\t1668701718\t768.00\t90.0\n",
    "4\t785.526262\t30211560.0\t28.0\tWon\tW\t10.0\t4.00\t2000.0\t640665\t577.00\t31.0\n",
    "...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\n",
    "181668\t102.482422\t30200854.0\t25.0\tWon\tW\t41.0\t0.96\t1220.0\t164141591\t591.00\t1.0\n",
    "181669\t208.086469\t30200854.0\t25.0\tWon\tW\t41.0\t0.95\t1500.0\t164141591\t589.00\t1.0\n",
    "181670\t4.235594\t30200854.0\t25.0\tWon\tW\t41.0\t0.71\t1250.0\t164141591\t619.00\t1.0\n",
    "181671\t30.365013\t30200854.0\t25.0\tWon\tW\t41.0\t0.85\t1250.0\t164141591\t601.00\t1.0\n",
    "181672\t406.686538\t30200854.0\t25.0\tWon\tW\t41.0\t0.71\t1240.0\t164141591\t607.00\t1.0\n",
    "181673 rows × 11 columns\n",
    "\n",
    "skewness and outliers\n",
    "#copy the dataframe to another to check the skewness and outliers\n",
    "\n",
    "df_1=df.copy()\n",
    "df_1.head()\n",
    "quantity tons\tcustomer\tcountry\tstatus\titem type\tapplication\tthickness\twidth\tproduct_ref\tselling_price\tdelivery_time_taken\n",
    "0\t54.151139\t30156308.0\t28.0\tWon\tW\t10.0\t2.00\t1500.0\t1670798778\t854.00\t91.0\n",
    "1\t768.024839\t30202938.0\t25.0\tWon\tW\t41.0\t0.80\t1210.0\t1668701718\t1047.00\t0.0\n",
    "2\t386.127949\t30153963.0\t30.0\tWon\tWI\t28.0\t0.38\t952.0\t628377\t644.33\t90.0\n",
    "3\t202.411065\t30349574.0\t32.0\tWon\tS\t59.0\t2.30\t1317.0\t1668701718\t768.00\t90.0\n",
    "4\t785.526262\t30211560.0\t28.0\tWon\tW\t10.0\t4.00\t2000.0\t640665\t577.00\t31.0\n",
    "#checking the skew for each features\n",
    "\n",
    "for col in ['quantity tons', 'customer', 'country', 'application', 'thickness', 'width','product_ref', 'selling_price','delivery_time_taken']:\n",
    "    print(col)\n",
    "    print(skew(df_1[col]))\n",
    "quantity tons\n",
    "424.68730994731504\n",
    "customer\n",
    "86.9845901502091\n",
    "country\n",
    "0.7538286488466676\n",
    "application\n",
    "0.7245583983073829\n",
    "thickness\n",
    "303.4450657126513\n",
    "width\n",
    "0.37459367901082125\n",
    "product_ref\n",
    "1.0152071910463223\n",
    "selling_price\n",
    "301.38559971625915\n",
    "delivery_time_taken\n",
    "0.46972996509615955\n",
    "# function for box plot, hist plot, violin plot to understand through visualize\n",
    "\n",
    "def plot(df, column):\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(1,3,1)\n",
    "    sns.boxplot(data=df, x=column)\n",
    "    plt.title(f'Box Plot for {column}')\n",
    "\n",
    "    plt.subplot(1,3,2)\n",
    "    sns.histplot(data=df, x=column, kde=True, bins=50)\n",
    "    plt.title(f'Distribution Plot for {column}')\n",
    "\n",
    "    plt.subplot(1,3,3)\n",
    "    sns.violinplot(data=df, x=column)\n",
    "    plt.title(f'Violin Plot for {column}')\n",
    "    plt.show()\n",
    "# plotting the features to check skew and outliers\n",
    "\n",
    "for i in ['quantity tons', 'customer', 'country', 'application', 'thickness', 'width','product_ref', 'selling_price','delivery_time_taken']:\n",
    "    plot(df_1, i)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# handling skew with log transformation method\n",
    "\n",
    "df_1['quantity_tons_log']=np.log(df_1['quantity tons'])\n",
    "df_1['thickness_log']=np.log(df_1['thickness'])\n",
    "df_1['selling_price_log']=np.log(df_1['selling_price'])\n",
    "df_1.head()\n",
    "quantity tons\tcustomer\tcountry\tstatus\titem type\tapplication\tthickness\twidth\tproduct_ref\tselling_price\tdelivery_time_taken\tquantity_tons_log\tthickness_log\tselling_price_log\n",
    "0\t54.151139\t30156308.0\t28.0\tWon\tW\t10.0\t2.00\t1500.0\t1670798778\t854.00\t91.0\t3.991779\t0.693147\t6.749931\n",
    "1\t768.024839\t30202938.0\t25.0\tWon\tW\t41.0\t0.80\t1210.0\t1668701718\t1047.00\t0.0\t6.643822\t-0.223144\t6.953684\n",
    "2\t386.127949\t30153963.0\t30.0\tWon\tWI\t28.0\t0.38\t952.0\t628377\t644.33\t90.0\t5.956169\t-0.967584\t6.468211\n",
    "3\t202.411065\t30349574.0\t32.0\tWon\tS\t59.0\t2.30\t1317.0\t1668701718\t768.00\t90.0\t5.310301\t0.832909\t6.643790\n",
    "4\t785.526262\t30211560.0\t28.0\tWon\tW\t10.0\t4.00\t2000.0\t640665\t577.00\t31.0\t6.666354\t1.386294\t6.357842\n",
    "df_1.drop(columns=['quantity tons','thickness','selling_price'], inplace=True)\n",
    "df_1.head()\n",
    "customer\tcountry\tstatus\titem type\tapplication\twidth\tproduct_ref\tdelivery_time_taken\tquantity_tons_log\tthickness_log\tselling_price_log\n",
    "0\t30156308.0\t28.0\tWon\tW\t10.0\t1500.0\t1670798778\t91.0\t3.991779\t0.693147\t6.749931\n",
    "1\t30202938.0\t25.0\tWon\tW\t41.0\t1210.0\t1668701718\t0.0\t6.643822\t-0.223144\t6.953684\n",
    "2\t30153963.0\t30.0\tWon\tWI\t28.0\t952.0\t628377\t90.0\t5.956169\t-0.967584\t6.468211\n",
    "3\t30349574.0\t32.0\tWon\tS\t59.0\t1317.0\t1668701718\t90.0\t5.310301\t0.832909\t6.643790\n",
    "4\t30211560.0\t28.0\tWon\tW\t10.0\t2000.0\t640665\t31.0\t6.666354\t1.386294\t6.357842\n",
    "#outliers handle with IQR and cliping method \n",
    "\n",
    "def outlier(df, column):\n",
    "    iqr = df[column].quantile(0.75) - df[column].quantile(0.25)\n",
    "    upper_threshold = df[column].quantile(0.75) + (1.5*iqr)\n",
    "    lower_threshold = df[column].quantile(0.25) - (1.5*iqr)\n",
    "    df[column] = df[column].clip(lower_threshold, upper_threshold)\n",
    "# using outlier function handle the outliers for the below features\n",
    "\n",
    "outlier(df_1,'quantity_tons_log')\n",
    "outlier(df_1,'thickness_log')\n",
    "outlier(df_1,'selling_price_log')\n",
    "outlier(df_1,'width')\n",
    "df_1\n",
    "customer\tcountry\tstatus\titem type\tapplication\twidth\tproduct_ref\tdelivery_time_taken\tquantity_tons_log\tthickness_log\tselling_price_log\n",
    "0\t30156308.0\t28.0\tWon\tW\t10.0\t1500.0\t1670798778\t91.0\t3.991779\t0.693147\t6.749931\n",
    "1\t30202938.0\t25.0\tWon\tW\t41.0\t1210.0\t1668701718\t0.0\t6.643822\t-0.223144\t6.953684\n",
    "2\t30153963.0\t30.0\tWon\tWI\t28.0\t952.0\t628377\t90.0\t5.956169\t-0.967584\t6.468211\n",
    "3\t30349574.0\t32.0\tWon\tS\t59.0\t1317.0\t1668701718\t90.0\t5.310301\t0.832909\t6.643790\n",
    "4\t30211560.0\t28.0\tWon\tW\t10.0\t1980.0\t640665\t31.0\t6.666354\t1.386294\t6.357842\n",
    "...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\n",
    "181668\t30200854.0\t25.0\tWon\tW\t41.0\t1220.0\t164141591\t1.0\t4.629691\t-0.040822\t6.381816\n",
    "181669\t30200854.0\t25.0\tWon\tW\t41.0\t1500.0\t164141591\t1.0\t5.337954\t-0.051293\t6.378426\n",
    "181670\t30200854.0\t25.0\tWon\tW\t41.0\t1250.0\t164141591\t1.0\t1.443523\t-0.342490\t6.428105\n",
    "181671\t30200854.0\t25.0\tWon\tW\t41.0\t1250.0\t164141591\t1.0\t3.413291\t-0.162519\t6.398595\n",
    "181672\t30200854.0\t25.0\tWon\tW\t41.0\t1240.0\t164141591\t1.0\t6.008043\t-0.342490\t6.408529\n",
    "181673 rows × 11 columns\n",
    "\n",
    "#plotting after skew and outliers \n",
    "\n",
    "for i in ['quantity_tons_log', 'thickness_log', 'width', 'selling_price_log']:\n",
    "    plot(df_1, i)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# verify any columns are highly correlated using Heatmap\n",
    "\n",
    "cor=df_1[['quantity_tons_log', 'customer', 'country', 'application', 'thickness_log', 'width','product_ref', 'selling_price_log','delivery_time_taken']].corr()\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(cor,annot=True)\n",
    "<Axes: >\n",
    "\n",
    "Regression model to predict selling price\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "df_2=df_1.copy()\n",
    "df_2\n",
    "customer\tcountry\tstatus\titem type\tapplication\twidth\tproduct_ref\tdelivery_time_taken\tquantity_tons_log\tthickness_log\tselling_price_log\n",
    "0\t30156308.0\t28.0\tWon\tW\t10.0\t1500.0\t1670798778\t91.0\t3.991779\t0.693147\t6.749931\n",
    "1\t30202938.0\t25.0\tWon\tW\t41.0\t1210.0\t1668701718\t0.0\t6.643822\t-0.223144\t6.953684\n",
    "2\t30153963.0\t30.0\tWon\tWI\t28.0\t952.0\t628377\t90.0\t5.956169\t-0.967584\t6.468211\n",
    "3\t30349574.0\t32.0\tWon\tS\t59.0\t1317.0\t1668701718\t90.0\t5.310301\t0.832909\t6.643790\n",
    "4\t30211560.0\t28.0\tWon\tW\t10.0\t1980.0\t640665\t31.0\t6.666354\t1.386294\t6.357842\n",
    "...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\n",
    "181668\t30200854.0\t25.0\tWon\tW\t41.0\t1220.0\t164141591\t1.0\t4.629691\t-0.040822\t6.381816\n",
    "181669\t30200854.0\t25.0\tWon\tW\t41.0\t1500.0\t164141591\t1.0\t5.337954\t-0.051293\t6.378426\n",
    "181670\t30200854.0\t25.0\tWon\tW\t41.0\t1250.0\t164141591\t1.0\t1.443523\t-0.342490\t6.428105\n",
    "181671\t30200854.0\t25.0\tWon\tW\t41.0\t1250.0\t164141591\t1.0\t3.413291\t-0.162519\t6.398595\n",
    "181672\t30200854.0\t25.0\tWon\tW\t41.0\t1240.0\t164141591\t1.0\t6.008043\t-0.342490\t6.408529\n",
    "181673 rows × 11 columns\n",
    "\n",
    "#checking values count to encoding the catogorical features\n",
    "\n",
    "df_2[\"status\"].value_counts()\n",
    "status\n",
    "Won                116012\n",
    "Lost                34438\n",
    "Not lost for AM     19573\n",
    "Revised              4276\n",
    "To be approved       4170\n",
    "Draft                3140\n",
    "Offered                53\n",
    "Offerable              10\n",
    "Wonderful               1\n",
    "Name: count, dtype: int64\n",
    "# using map function encoding the values in status column\n",
    "\n",
    "df_2[\"status\"]= df_2[\"status\"].map({'Won':1, 'Draft':2, 'To be approved':3, 'Lost':0, 'Not lost for AM':4,\n",
    "                                'Wonderful':5, 'Revised':6, 'Offered':7, 'Offerable':8})\n",
    "df_2['status'].unique()\n",
    "array([1, 2, 3, 0, 4, 5, 6, 7, 8], dtype=int64)\n",
    "#checking values count to encoding the catogorical features\n",
    "\n",
    "df_2['item type'].value_counts()\n",
    "item type\n",
    "W         105615\n",
    "S          69236\n",
    "PL          5660\n",
    "Others       610\n",
    "WI           524\n",
    "IPL           27\n",
    "SLAWR          1\n",
    "Name: count, dtype: int64\n",
    "# using ordinalencoder to encode the values in item type column\n",
    "\n",
    "df_2[\"item type\"]= OrdinalEncoder().fit_transform(df_2[[\"item type\"]])\n",
    "df_2['item type'].unique()\n",
    "array([5., 6., 3., 1., 2., 0., 4.])\n",
    "df_2.head()\n",
    "customer\tcountry\tstatus\titem type\tapplication\twidth\tproduct_ref\tdelivery_time_taken\tquantity_tons_log\tthickness_log\tselling_price_log\n",
    "0\t30156308.0\t28.0\t1\t5.0\t10.0\t1500.0\t1670798778\t91.0\t3.991779\t0.693147\t6.749931\n",
    "1\t30202938.0\t25.0\t1\t5.0\t41.0\t1210.0\t1668701718\t0.0\t6.643822\t-0.223144\t6.953684\n",
    "2\t30153963.0\t30.0\t1\t6.0\t28.0\t952.0\t628377\t90.0\t5.956169\t-0.967584\t6.468211\n",
    "3\t30349574.0\t32.0\t1\t3.0\t59.0\t1317.0\t1668701718\t90.0\t5.310301\t0.832909\t6.643790\n",
    "4\t30211560.0\t28.0\t1\t5.0\t10.0\t1980.0\t640665\t31.0\t6.666354\t1.386294\t6.357842\n",
    "# function to choose the best algorithm\n",
    "\n",
    "def best_ML_algorithm(df,algorithms):\n",
    "        x=df.drop(columns=['selling_price_log'],axis=1)\n",
    "        y=df['selling_price_log']\n",
    "\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "        results={}\n",
    "\n",
    "        for algorithm in algorithms:\n",
    "\n",
    "                model=algorithm().fit(x_train,y_train)\n",
    "                y_pred=model.predict(x_test)\n",
    "                \n",
    "                MAE=metrics.mean_absolute_error(y_test, y_pred)\n",
    "                MSE=metrics.mean_squared_error(y_test, y_pred)\n",
    "                RMSE=np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "                r2 = metrics.r2_score(y_test, y_pred)\n",
    "\n",
    "                results[algorithm.__name__] = {'Mean Absolute Error':MAE,'Mean Squared Error':MSE,'Root Mean Squared Error':RMSE,\n",
    "                                        'R² score':r2}\n",
    "                \n",
    "        best_algorithm = max(results.items(), key=lambda item: item[1]['R² score'])[0]  \n",
    "        \n",
    "        return best_algorithm, results        \n",
    "# using above function try to identify the best algorithm\n",
    "\n",
    "algorithms = [LinearRegression, DecisionTreeRegressor, ExtraTreesRegressor, RandomForestRegressor,\n",
    "            GradientBoostingRegressor, XGBRegressor]\n",
    "best_algorithm, results = best_ML_algorithm(df_2, algorithms)\n",
    "\n",
    "results_df = pd.DataFrame(results).transpose()\n",
    "\n",
    "print(\"**Evaluation Metrics for All Algorithms:**\")\n",
    "print(results_df.to_string())\n",
    "\n",
    "print(\"**Best Algorithm:**\",best_algorithm)\n",
    "**Evaluation Metrics for All Algorithms:**\n",
    "                           Mean Absolute Error  Mean Squared Error  Root Mean Squared Error  R² score\n",
    "LinearRegression                      0.162836            0.045489                 0.213281  0.302651\n",
    "DecisionTreeRegressor                 0.054141            0.010117                 0.100583  0.844905\n",
    "ExtraTreesRegressor                   0.046194            0.005703                 0.075518  0.912573\n",
    "RandomForestRegressor                 0.045846            0.005531                 0.074373  0.915203\n",
    "GradientBoostingRegressor             0.118057            0.021458                 0.146487  0.671040\n",
    "XGBRegressor                          0.079373            0.011073                 0.105230  0.830243\n",
    "**Best Algorithm:** RandomForestRegressor\n",
    "# hyper parameter tuning to identify the best parameter using gridsearch cv\n",
    "\n",
    "x=df_2.drop(columns=['selling_price_log'],axis=1)\n",
    "y=df_2['selling_price_log']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "param_grid = { \n",
    "    'n_estimators' :[100,150],\n",
    "    'max_depth': [10,20],     \n",
    "    'min_samples_split': [2,4], \n",
    "    'min_samples_leaf': [1,2],\n",
    "    'max_features'    : ['sqrt', 'log2', None] }\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(estimator=RandomForestRegressor(),\n",
    "                            param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "grid_search.best_params_,grid_search.best_score_\n",
    "({'max_depth': 20,\n",
    "  'max_features': None,\n",
    "  'min_samples_leaf': 1,\n",
    "  'min_samples_split': 2,\n",
    "  'n_estimators': 150},\n",
    " 0.8954315247195281)\n",
    "# with best algorithm matching the parameter from above result and train the model\n",
    "\n",
    "x=df_2.drop(columns=['selling_price_log'],axis=1)\n",
    "y=df_2['selling_price_log']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "model_regressor = RandomForestRegressor(n_estimators=150,max_features=None,max_depth=20,min_samples_leaf=1,min_samples_split=2,random_state=42)\n",
    "model_regressor.fit(x_train, y_train)\n",
    "\n",
    "y_pred = model_regressor.predict(x_test)\n",
    "\n",
    "# evaluation metrics \n",
    "MAE=metrics.mean_absolute_error(y_test, y_pred)\n",
    "MSE = metrics.mean_squared_error(y_test, y_pred)\n",
    "RMSE=np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "r2 = metrics.r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error: {MAE}\")\n",
    "print(f\"Mean Squared Error: {MSE}\")\n",
    "print(f\"Root Mean Squared Error: {RMSE}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "Mean Absolute Error: 0.04865670494149515\n",
    "Mean Squared Error: 0.005851891925041096\n",
    "Root Mean Squared Error: 0.07649765960499116\n",
    "R-squared: 0.9102894678604176\n",
    "#checkig the features scores and importants\n",
    "\n",
    "feature_scores = pd.Series(model_regressor.feature_importances_, index=x_train.columns).sort_values(ascending=False)\n",
    "feature_scores\n",
    "product_ref            0.295662\n",
    "delivery_time_taken    0.245779\n",
    "status                 0.173063\n",
    "country                0.072261\n",
    "customer               0.067352\n",
    "thickness_log          0.043541\n",
    "quantity_tons_log      0.042084\n",
    "application            0.023018\n",
    "item type              0.019436\n",
    "width                  0.017804\n",
    "dtype: float64\n",
    "# store the model using pickle\n",
    "\n",
    "with open('Regressor.pkl','wb') as files:\n",
    "    pickle.dump(model_regressor,files)\n",
    "#load the pickle and predict the selling price with regressor model\n",
    "\n",
    "with open('Regressor.pkl','rb') as files:\n",
    "    predict_model=pickle.load(files)\n",
    "\n",
    "pre=predict_model.predict(np.array([[30156308.0,28.0,1,5.0,10.0,1500.0,1670798778,91.0,3.991779,0.693147]]))\n",
    "np.exp(pre[0])\n",
    "779.8488645142141\n",
    "Classifier Model to predict the Status\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier,RandomForestClassifier,GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "#filter out the values (won|lost) in status column\n",
    "\n",
    "df_c=df_2[(df_2.status==1)|(df_2.status==0)]\n",
    "df_c\n",
    "customer\tcountry\tstatus\titem type\tapplication\twidth\tproduct_ref\tdelivery_time_taken\tquantity_tons_log\tthickness_log\tselling_price_log\n",
    "0\t30156308.0\t28.0\t1\t5.0\t10.0\t1500.0\t1670798778\t91.0\t3.991779\t0.693147\t6.749931\n",
    "1\t30202938.0\t25.0\t1\t5.0\t41.0\t1210.0\t1668701718\t0.0\t6.643822\t-0.223144\t6.953684\n",
    "2\t30153963.0\t30.0\t1\t6.0\t28.0\t952.0\t628377\t90.0\t5.956169\t-0.967584\t6.468211\n",
    "3\t30349574.0\t32.0\t1\t3.0\t59.0\t1317.0\t1668701718\t90.0\t5.310301\t0.832909\t6.643790\n",
    "4\t30211560.0\t28.0\t1\t5.0\t10.0\t1980.0\t640665\t31.0\t6.666354\t1.386294\t6.357842\n",
    "...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\n",
    "181668\t30200854.0\t25.0\t1\t5.0\t41.0\t1220.0\t164141591\t1.0\t4.629691\t-0.040822\t6.381816\n",
    "181669\t30200854.0\t25.0\t1\t5.0\t41.0\t1500.0\t164141591\t1.0\t5.337954\t-0.051293\t6.378426\n",
    "181670\t30200854.0\t25.0\t1\t5.0\t41.0\t1250.0\t164141591\t1.0\t1.443523\t-0.342490\t6.428105\n",
    "181671\t30200854.0\t25.0\t1\t5.0\t41.0\t1250.0\t164141591\t1.0\t3.413291\t-0.162519\t6.398595\n",
    "181672\t30200854.0\t25.0\t1\t5.0\t41.0\t1240.0\t164141591\t1.0\t6.008043\t-0.342490\t6.408529\n",
    "150450 rows × 11 columns\n",
    "\n",
    "#checking balance of the status values\n",
    "\n",
    "df_c['status'].value_counts()\n",
    "status\n",
    "1    116012\n",
    "0     34438\n",
    "Name: count, dtype: int64\n",
    "ax=df_c['status'].value_counts().plot.pie(autopct='%.2f')\n",
    "\n",
    "# oversampling the data using SMOTE \n",
    "\n",
    "x=df_c.drop('status',axis=1)\n",
    "y=df_c['status']\n",
    "\n",
    "smote= SMOTE()\n",
    "\n",
    "x_smote, y_smote = smote.fit_resample(x,y) \n",
    "\n",
    "ax=y_smote.value_counts().plot.pie(autopct='%.2f')\n",
    "ax.set_title('over oversampling using smote')\n",
    "Text(0.5, 1.0, 'over oversampling using smote')\n",
    "\n",
    "#function to get the best classifier algorithm\n",
    "\n",
    "def best_ML_class_algo(x_smote,y_smote,algorithms):\n",
    "        \n",
    "        x_train, x_test, y_train, y_test = train_test_split(x_smote, y_smote, test_size=0.3, random_state=42)\n",
    "\n",
    "        results={}\n",
    "\n",
    "        for algorithm in algorithms:\n",
    "\n",
    "                model=algorithm().fit(x_train,y_train)\n",
    "                y_pred=model.predict(x_test)\n",
    "\n",
    "                accuracy =metrics. accuracy_score(y_test, y_pred)\n",
    "                precision =metrics. precision_score(y_test, y_pred)\n",
    "                recall =metrics. recall_score(y_test, y_pred)\n",
    "                f1 = metrics.f1_score(y_test, y_pred)\n",
    "\n",
    "                results[algorithm.__name__] = {'Accuracy score': accuracy,'Precision': precision,'Recall': recall,'F1-score': f1}\n",
    "                \n",
    "        best_algorithm = max(results.items(), key=lambda item: item[1]['Accuracy score'])[0]  \n",
    "        \n",
    "        return best_algorithm, results  \n",
    "# using above function try to identify the best algorithm\n",
    "\n",
    "algorithms = [ DecisionTreeClassifier, ExtraTreesClassifier, RandomForestClassifier,\n",
    "            GradientBoostingClassifier, XGBClassifier]\n",
    "best_algorithm, results = best_ML_class_algo(x_smote,y_smote, algorithms)\n",
    "\n",
    "results_df = pd.DataFrame(results).transpose()\n",
    "\n",
    "print(\"**Evaluation Metrics for All Algorithms:**\")\n",
    "print(results_df.to_string())\n",
    "\n",
    "print(\"**Best Algorithm:**\",best_algorithm)\n",
    "**Evaluation Metrics for All Algorithms:**\n",
    "                            Accuracy score  Precision    Recall  F1-score\n",
    "DecisionTreeClassifier            0.955436   0.956182  0.954082  0.955131\n",
    "ExtraTreesClassifier              0.977589   0.987346  0.967317  0.977229\n",
    "RandomForestClassifier            0.975419   0.985162  0.965092  0.975024\n",
    "GradientBoostingClassifier        0.801230   0.817695  0.772374  0.794389\n",
    "XGBClassifier                     0.933528   0.951721  0.912585  0.931742\n",
    "**Best Algorithm:** ExtraTreesClassifier\n",
    "# hyper parameter tuning to identify the best parameter using gridsearch cv\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_smote, y_smote, test_size=0.3, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators' :[50,100],\n",
    "    'max_depth': [ 10,20,25],     \n",
    "    'min_samples_split': [2, 4],  \n",
    "    'min_samples_leaf': [1, 2]     }\n",
    "\n",
    "\n",
    "model = ExtraTreesClassifier(random_state=42) \n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring=\"accuracy\",n_jobs=-1)\n",
    "\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "grid_search.best_params_,grid_search.best_score_\n",
    "({'max_depth': 25,\n",
    "  'min_samples_leaf': 1,\n",
    "  'min_samples_split': 2,\n",
    "  'n_estimators': 100},\n",
    " 0.9622635744832578)\n",
    "# with best algorithm matching the parameter from bove result and train the model\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_smote, y_smote, test_size=0.3, random_state=42)\n",
    "\n",
    "model_classifier = ExtraTreesClassifier(n_estimators=100,max_depth=25,min_samples_leaf=1,min_samples_split=2)\n",
    "model_classifier.fit(x_train, y_train)\n",
    "\n",
    "y_pred = model_classifier.predict(x_test)\n",
    "\n",
    "# evaluation metrics\n",
    "print(metrics.confusion_matrix(y_test,y_pred))\n",
    "print(metrics.classification_report(y_test,y_pred))\n",
    "[[34633   370]\n",
    " [ 1988 32617]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.95      0.99      0.97     35003\n",
    "           1       0.99      0.94      0.97     34605\n",
    "\n",
    "    accuracy                           0.97     69608\n",
    "   macro avg       0.97      0.97      0.97     69608\n",
    "weighted avg       0.97      0.97      0.97     69608\n",
    "\n",
    "# Receiver Operating Characteristic (ROC) Curve and Area Under the Curve (AUC)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test,y_pred)\n",
    "auc_curve=auc(x=fpr,y=tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % auc_curve)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='No Skill')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "#checkig the features scores and importants\n",
    "\n",
    "feature_scores = pd.Series(model_classifier.feature_importances_, index=x_train.columns).sort_values(ascending=False)\n",
    "feature_scores\n",
    "delivery_time_taken    0.180634\n",
    "item type              0.130623\n",
    "customer               0.125023\n",
    "country                0.116439\n",
    "application            0.108861\n",
    "quantity_tons_log      0.090905\n",
    "selling_price_log      0.077596\n",
    "product_ref            0.064204\n",
    "thickness_log          0.063402\n",
    "width                  0.042312\n",
    "dtype: float64\n",
    "# store the model using pickle\n",
    "\n",
    "with open('Classifier.pkl','wb') as  files:\n",
    "    pickle.dump(model_classifier,files)\n",
    "# load the pickle and predict the status with classifier model\n",
    "\n",
    "with open('Classifier.pkl','rb') as files:\n",
    "    clas_model=pickle.load(files)\n",
    "\n",
    "cla = clas_model.predict( np.array([[30156308.0,28.0,5.0,10.0,1500.0,1670798778,91.0,3.991779,0.693147,6.749931]]))\n",
    "if cla[0] == 1:\n",
    "    print('Won')\n",
    "else:\n",
    "    print('Lose')\n",
    "Won"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
